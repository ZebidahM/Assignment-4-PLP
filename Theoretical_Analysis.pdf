Part 1
    Question 1
        Q1: Explain how AI-driven code generation tools (e.g., GitHub Copilot) reduce development time. What are their limitations?
            AI-driven code generation tools like GitHub Copilot reduce development time by all the following:
            1. Code completion: this represents the most immediate time-saver. When developers have a clear mental model of what they want to implement, translating those thoughts into syntactically correct code often becomes the bottleneck. These tools anticipate coding patterns and suggest implementations that align with developer intent, allowing programmers to review and accept suggestions rather than typing everything from scratch. This pattern recognition extends beyond simple autocomplete to understanding context, function signatures, and common algorithmic approaches.
            2. Boilerplate generation: this provides another significant efficiency gain. Most of the software development process involves repetitive patterns setting up API endpoints, database connections, test scaffolding, or configuration files. AI tools can generate these standard structures instantly, freeing developers to focus on unique business logic rather than routine setup code.
            3. The tools also serve as intelligent documentation, suggesting appropriate variable names, function signatures, and even inline comments that follow established conventions. This reduces the cognitive load of maintaining consistent coding standards across a project.
            Its primary limitations:
            Scale constraints represent a fundamental limitation. These tools excel at generating code snippets, functions, or small modules, but struggle with architectural decisions or large scale system design. They cannot understand complex business requirements, make technology stack decisions, or orchestrate multi-component interactions that span entire applications.
            Dependency risks emerge when developers, particularly those early in their careers, lean too heavily on AI suggestions without developing underlying programming fundamentals. This can create a gap between what developers can produce with AI assistance versus their independent problem-solving capabilities. Over-reliance may also lead to acceptance of suboptimal solutions simply because they're suggested, rather than thinking critically about implementation approaches.
            Quality and context awareness issues also persist. AI tools may suggest code that works but isn't optimal for the specific use case, doesn't follow project-specific patterns, or introduces subtle bugs that require debugging time that offsets some efficiency gains.

        Q2: Compare supervised and unsupervised learning in the context of automated bug detection.
            Supervised learning requires labeled datasets where code samples are explicitly marked as buggy or clean. Based on this, the model learns to recognise patterns in buggy codes and correlate with the known bug types. It excels at detecting specific bug patterns it has seen before. Once trained on sufficient examples of buffer overflows or SQL injection vulnerabilities, supervised models can reliably flag similar issues in new code. They also provide more interpretable results since they're trained to recognize specific problem patterns that developers can understand and address. While unsupervised learning identifies anomalies by learning normal code patterns without requiring pre-labeled bug examples. These systems establish baselines for typical coding behavior (variable usage patterns, function call sequences, or structural relationships) then flag deviations from these norms as potentially problematic.
            Supervised approaches face significant data challenges. Creating comprehensive labeled datasets requires extensive manual effort from experienced developers who must review code and accurately identify bugs. This process is time intensive and potentially introduces labeling inconsistencies. More critically, supervised models struggle with novel bug types they haven't encountered during training, limiting their ability to catch emerging vulnerability patterns or unique logical errors while unsupervised models flag anomalies rather than confirmed bugs. They often identify legitimate but unusual coding patterns as suspicious. This creates noise that developers must filter through, potentially reducing trust in the system. Additionally, these models may miss bugs that follow common coding patterns but contain subtle logical errors.
            In practice, effective automated bug detection systems often combine both approaches. Supervised learning handles well-understood vulnerability categories with high precision, while unsupervised methods provide broader coverage for novel or complex issues. The choice between approaches also depends on available resources. Organizations with extensive historical bug data may favor supervised methods, while those seeking comprehensive coverage without labeling overhead might prefer unsupervised approaches.

        Q3: Why is bias mitigation critical when using AI for user experience personalization?
            Bias mitigation is essential in AI-driven user experience personalization because biased algorithms can perpetuate discrimination, limit user opportunities, and ultimately undermine the very personalization goals they're designed to achieve. AI personalization systems often reflect and amplify existing societal biases present in their training data. When these systems learn from historical user behavior that contains discriminatory patterns, they can systematically disadvantage certain demographic groups. Biased personalization algorithms can trap users in narrow information environments that reinforce existing beliefs and preferences while limiting exposure to diverse perspectives. In personalization systems that control access to opportunities - such as educational content, financial products, or employment listings - bias can create systematic disadvantages for certain user groups.Bias ultimately undermines personalization effectiveness by making decisions based on stereotypes rather than individual user characteristics. 

    Question 2
        How does AIOps improve software deployment efficiency? Provide two examples.
            AIOps improves software deployment efficiency by automating manual processes and using predictive analytics to prevent failures before they occur. The technology leverages machine learning models trained on historical data to predict potential issues during deployment and optimize the entire process for better outcomes.
            Two examples:
            1. Automated Rollback: Harness uses AI to automatically roll back failed deployments, minimizing the need for human intervention AI-Powered DevOps: Automating Software Development and Deployment, while CircleCI uses AI to optimize test execution by prioritizing tests with the best efficiency rates based on historical data.
            2. Predictive Failure Prevention: AI-driven systems predict potential build failures in advance by analyzing historical deployment data, identifying problems before they occur and reducing risks during live deployments.